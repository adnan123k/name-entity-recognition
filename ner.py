# -*- coding: utf-8 -*-
"""ner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IGCHUw5QaTefyO990dLIKm9kYjZ5gghQ
"""

!pip install transformers datasets

import transformers
import datasets
import numpy as np
import keras

raw_dataset=datasets.load_dataset("conll2003")

tokenizer=transformers.AutoTokenizer.from_pretrained("distilbert-base-uncased")

t=tokenizer(raw_dataset["train"][0]["tokens"],is_split_into_words=True)

t.tokens()

labels_name=raw_dataset["train"].features["ner_tags"].feature.names

raw_dataset["train"][0]["ner_tags"]

begin2inside={
    1:2,
    3:4,
    5:6,
    7:8
}

def align_targets(labels,word_ids):
  aligned_labels=[]
  last_word=None
  for word in word_ids:
    if word is None:
      label=-100
    elif word!=last_word:
      label=labels[word]
    else:
       label=labels[word]
       if label in begin2inside:
         label=begin2inside[label]
    aligned_labels.append(label) 
    last_word=label    
  return  aligned_labels

def tokenize_fn(batch):
  token_input=tokenizer(batch["tokens"],is_split_into_words=True,truncation=True)
  labels_batch=batch["ner_tags"]
  aligned_labels_batch=[]
  for i,labels in enumerate(labels_batch):
    words_ids=token_input.word_ids(i)
    aligned_labels_batch.append(align_targets(labels,words_ids))
  token_input["labels"]=aligned_labels_batch
  return token_input

tokenized_dataset=raw_dataset.map(tokenize_fn,batched=True,remove_columns=raw_dataset["train"].column_names)

!pip install seqeval

metrics=datasets.load_metric("seqeval")

def compute_metrics(logits_and_labels):
  logits,labels=logits_and_labels
  preds=np.argmax(logits,axis=-1)
  str_label=[[labels_name[t] for t in label if t!=-100] for label in labels]
  str_pred=[[labels_name[p] for p,t in zip(pred,target) if t!=-100]for pred,target in zip(preds,labels)]
  the_metrics=metrics.compute(predications=str_pred,references=str_label)
  return {
      "precision":the_metrics["overall_precision"],
      "recall":the_metrics["overall_recall"],
      "f1":the_metrics["overall_f1"],
      "accuracy":the_metrics["overall_accuracy"]
  }

id2label={k:v for k,v in enumerate(labels_name)}
label2id={v:k for k,v in id2label.items()}

model=transformers.AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased",id2label=id2label,label2id=label2id)

training_args=transformers.TrainingArguments(
    "distilbert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    weight_decay=0.01,
    learning_rate=2e-5
)

data_collector=transformers.DataCollatorForTokenClassification(tokenizer=tokenizer)

train=transformers.Trainer(model,
                           args=training_args,
                           train_dataset=tokenized_dataset["train"],
                           eval_dataset=tokenized_dataset["validation"],
                           tokenizer=tokenizer,
                           data_collator=data_collector,
                           compute_metrics=compute_metrics)

train.train()

